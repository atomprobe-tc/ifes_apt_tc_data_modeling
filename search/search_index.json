{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Documentation for ifes_apt_tc_data_modeling","text":"<p>A Python software module for reading file formats that are used in the research field of atom probe tomography and field ion microscopy.</p> Project and community <p>For questions or suggestions:</p> <ul> <li>Open an issue on the repository via GitHub.</li> </ul> <p>The work is supported by the International Field Emission Society (IFES) Atom Probe Tomography Technical Committee (APT TC).</p> <p>The work is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - 460197019 (FAIRmat).</p> <p>The library is used for the pynxtools-apm to standardized atom probe data using NeXus.</p>"},{"location":"index.html#tutorial","title":"Tutorial","text":"<p>Installation guides exist for users and developers.</p> <ul> <li>Install for users</li> <li>Install for developers</li> <li>Compile the documentation</li> </ul>"},{"location":"index.html#how-to-guides","title":"How-to guides","text":""},{"location":"index.html#learn","title":"Learn","text":"<p>Background knowledge to specific topics.</p> <ul> <li>Exchange</li> <li>Provenance</li> <li>Specifications</li> <li>NeXus NXapm</li> </ul>"},{"location":"index.html#reference","title":"Reference","text":""},{"location":"explanation/learn.html","title":"Exchange","text":""},{"location":"explanation/learn.html#calling-for-more-exchange-about-metadata-in-the-field","title":"Calling for more exchange about (meta)data in the field","text":"<p>The proprietary software package IVAS now APSuite by AMETEK/Cameca is the workhorse for data acquisition and analysis in the atom probe community. Several file formats that this software uses are proprietary with the <code>.apt</code> file format as one exception. The idea of the <code>.apt</code> file format is opening up atom probe data from Cameca instrument, i.e., Local Electrode Atom Probe (LEAP) instruments for scientific data analysis and public dissemination. The International Field Emission Society (IFES) and Cameca have worked together to communicate a documentation for the format which enabled the community to develop open-source reading capabilities as implemented also in the <code>ifes_apt_tc_data_modeling</code> library through its <code>apt</code> module.</p> <p>While <code>.apt</code> is more and more getting accepted, traditional text and binary file formats are still commonly used in daily atom probe research practice. Not for all of these formats formal specifications exist. This makes working with these formats in software tools other than from AMETEK/Cameca trickier and error-prone.</p> <p>A practical solution to raise at least awareness of this problem has been that scientists collect examples (instances) of files in respective formats. Pieces of information about the content and formatting of atom probe file formats were reported in the literature (e.g. in the books by D. Larson et al. or B. Gault et al.. Atom probers like Daniel Haley have contributed substantially through raising awareness of the issue within the community. Consequently, individuals of the community invested into reverse engineering efforts about what these formats store and how this can be parsed using open-source software that is developed within the atom probe community and beyond.</p> <p>The <code>ifes_apt_tc_data_modeling</code> library bundles this knowledge highlighting though also that there are still gaps in our understanding. From an academic point of view these should be closed so that whenever possible atom probe data and metadata can be always communicated clearly with respect to what do certain numbers mean, i.e., what are the semantics and concepts behind the numbers and data items. </p> <p>As an example, the <code>.pos</code> file format stores a table of number quadruples which mostly are interpreted as reconstructed position and mass-to-charge-state ratio values. Often the latter column is hijacked though to report conceptually different quantities like identifier used to distinguish clusters of atoms. Which specific input data were used, which parameterization was used for the reconstruction algorithm whose results were stored in that <code>.pos</code> file. These questions pertaining to the workflow and provenance along the data lifecycle remain unaddressed. Other technical issues exist with file formats like <code>.pos</code> and <code>.epos</code>: These do not provide a magic number that identifies the file as a true <code>.pos</code> file such that software tools and humans could make substantiated assumptions.</p> <p>Needs for improvement exist also for ranging definitions file formats like the commonly used <code>.rrng</code>, <code>.rng</code>, and <code>.env</code> formats: These merely store the resulting ranging definitions but do not store details based on which peak finding algorithm or even which mass-to-charge-state-ratio value array they were defined with. A more detailed discussion of these limitations is provided in the literature.</p> <p>The <code>ifes_apt_tc_data_modeling</code> library was developed after observing that many researchers in atom probe uses custom written code for reading atom probe data via classical file formats. While for several formats this is a rather simple programming exercise, it led though to parallel developments and many implementations that target only specific use cases instead of a general enough implementations with functionalities for all possible elements, ion types, and edge cases.</p>"},{"location":"explanation/nxapm.html","title":"NeXus NXapm","text":""},{"location":"explanation/nxapm.html#towards-a-global-data-model-for-atom-probe-research","title":"Towards a global data model for atom probe research","text":"<p>Recently, coordinated efforts that have build on the collective knowledge of the atom probe community have resulted in the development of a covering data model for atom probe tomography and related field ion microscopy. This model uses the NeXus data modeling framework. The proposed data model <code>NXapm</code> captures all aspects of data acquisition and data analysis to arrive at calibrated reconstructed datasets with applied ranging definitions with transparent communication about peak fitting and analysis routines. The proposal has recently been proposed for standardization with the NeXus International Advisory Committee (NIAC). The proposal was successful. In effect, <code>NXapm</code> is an official part of the NeXus standard. This is the respective pull request. This is the latest release of the standard.</p> <p>The activities have been acknowledged by key members of the atom probe community thus qualifying as suggested to be used global reporting standard for atom probe.</p> <p>With pynxtools-apm a reference implementation exists, that maps file formats in atom probe on NeXus/HDF5 files that follow the <code>NXapm</code> application definition. CamecaROOT-based file formats are currently not mapped. With pynxtools also a stand-alone software is available to validate these NeXus/HDF5 files against the <code>NXapm</code> standard.</p>"},{"location":"explanation/provenance.html","title":"Provenance","text":""},{"location":"explanation/provenance.html#provenance-and-how-to-improve-on-it","title":"Provenance and how to improve on it","text":"<p>Atom probers should be aware that file formats like <code>.pos</code>, <code>.epos</code>, or <code>.apt</code> are neither raw data nor follow a clear technical documentation that is completely available to the public. Therefore, all current file formats are not meeting the FAIR principles.</p> <p>Instead, share <code>.rraw</code>, <code>.str</code>, <code>.rhit</code>, and <code>.hits</code> files when working with AMETEK/Cameca instruments. Ideally, you add unique identifiers (such as SHA256 checksums) for each file. A documentation how you can do this was issued by your IFES APT TC colleagues (How to hash your data).</p>"},{"location":"explanation/suggestions.html","title":"Specifications","text":""},{"location":"explanation/suggestions.html#improve-on-building-and-communicating-file-format-specifications","title":"Improve on building and communicating file format specifications","text":"<p>File formats, data models, in (almost every) research field may not be fully documented. A checklist of the necessary pieces of information and documentation required to call a data model, data schema, and/or file format fully documented in accordance with the FAIR data and research software stewardship principles is given below:</p> <ol> <li>Each piece of information (bit/byte) is documented.</li> <li>This documentation fulfills the FAIR principles, i.e.    Wilkinson et al., 2016 and    Barker et al., 2022    For binary files, tools like kaitai struct offer a    solution to describe the exact binary information content in a data    item. This can be a file but also the storage of a database entry or the    response of a call to an API.    Let alone the binary structure is insufficient tough.</li> <li>To each piece of information there has to exist also a parameterized description,    what this piece of information conceptually means. One way to arrive at such    description is to use a data schema or ontology.    It is important to mention that the concepts in this schema/ontology have    unique identifier so that each data item/piece of information is identifiable    as an instance of an entry in a database or a knowledge graph.    This holds independently of which research data management system    or electronic lab notebook is used.</li> <li>In addition, it is very useful if timestamps are associated with each data item    (ISO8061 including time zone information) so that it is possible to create a    timeline of the context in which and when the e.g. file was created.</li> </ol> <p>The first and second point is known as a specification, while the third and fourth point emphasize that the contextualization and provenance is key to make a specification complete and useful.</p>"},{"location":"tutorial/compile_docs.html","title":"Compile the documentation","text":"<p>Provided <code>ifes_apt_tc_data_modeling</code> library was installed with the optional dependency <code>docs</code>, the documentation can be compiled locally using <code>mkdocs</code></p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"tutorial/install_devs.html","title":"Installation as a developer","text":"<p>Developers should clone the repository and install using e.g., uv, a Python virtual environment (venv),  or a conda environment. Exemplified for a Python venv this reads</p> <pre><code>git clone https://www.github.com/atomprobe-tc/ifes_apt_tc_data_modeling.git\ncd ifes_apt_tc_data_modeling\npython -m pip install --upgrade pip\npython -m pip install -e \".[dev,docs,ipynb]\"\n</code></pre> <p>This will install modules for linting, code styling, and unit testing via the pytest framework.</p> <p>Unit tests can then be started from the root directory of the installation with the following call:</p> <pre><code>pytest -sv tests\n</code></pre>"},{"location":"tutorial/install_user.html","title":"Installation as a user","text":"<p>It is recommended to use python 3.13 with a dedicated virtual environment for this package. Learn how to manage python versions and virtual environments.</p> <p>There are many alternatives to managing virtual environments and package dependencies (requirements). We recommend using <code>uv</code>, an extremely fast manager Python package and project manager. In this tutorial, you will find paralleled descriptions, using either <code>uv</code> or a more classical approach using <code>venv</code> and <code>pip</code>. Using a conda environment is yet another alternative.</p> <p>The library can be used on Windows, Mac, or Unix, provided a Python version is installed.</p>"},{"location":"tutorial/install_user.html#setup","title":"Setup","text":"<p>Start by creating a virtual environment, e.g., in a directory on your local computer:</p> uvvenvconda <p><code>uv</code> is capable of creating a virtual environment and install the required Python version at the same time.</p> <pre><code>uv venv --python 3.13\n</code></pre> <p>Note that you will need to install the Python version manually beforehand.</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>Note that you will need to install the Python version manually beforehand.</p> <pre><code>conda create -n venv python=3.13\nconda activate venv\nconda install pip\n</code></pre> <p>That command creates a new virtual environment in a directory called <code>.venv</code>.</p>"},{"location":"tutorial/install_user.html#installation","title":"Installation","text":"<p>Install the latest stable version of this package from PyPI with</p> uvpipconda <pre><code>uv pip install ifes_apt_tc_data_modeling[ipynb]\n</code></pre> <pre><code>pip install ifes_apt_tc_data_modeling[ipynb]\n</code></pre> <pre><code>python -m pip install ifes_apt_tc_data_modeling[ipynb]\n</code></pre> <p>This will install the module and jupyterlab whereby the notebook with examples become executable. </p>"},{"location":"tutorial/install_user.html#start-using-ifes_apt_tc_data_modeling","title":"Start using <code>ifes_apt_tc_data_modeling</code>","text":"<p>The jupyterlab server is started with</p> <pre><code>jupyter-lab\n</code></pre> <p>The notebook to run is the following <code>examples/ExamplesForUsersOrDevelopers.ipynb</code> That's it! You can now use the library that you have installed!</p>"}]}